{"cells":[{"cell_type":"markdown","metadata":{"id":"l-TkVDbRWfGo"},"source":["#MountDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35751,"status":"ok","timestamp":1706366681659,"user":{"displayName":"MARCO MILONE","userId":"17232021698904427689"},"user_tz":-60},"id":"VxNm4Ce28DUV","outputId":"4aad8e5d-e51d-4b7e-8408-db29da1c5baa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bn0Ur9P48JLp"},"outputs":[],"source":["import os\n","# Working Directory\n","os.chdir('/content/drive/MyDrive/ML_Colab/AV')"]},{"cell_type":"markdown","source":["#UnZip"],"metadata":{"id":"kq8F8ei-r9Mk"}},{"cell_type":"code","source":["# Function for unzip the dataset in the selected folder\n","import zipfile\n","from tqdm import tqdm\n","import os\n","\n","def unzip_file(zip_path, extract_to):\n","    \"\"\"\n","    Unzip a file to the specified directory using tqdm for progress tracking.\n","\n","    Parameters:\n","    - zip_path (str): Path to the zip file.\n","    - extract_to (str): Directory where the contents will be extracted.\n","    \"\"\"\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        # Get the total number of entries in the zip file for tqdm\n","        total_entries = sum(1 for _ in zip_ref.infolist())\n","\n","        # Set up tqdm for progress tracking\n","        with tqdm(total=total_entries, desc=\"Extracting\", unit=\"file\") as pbar:\n","            for entry in zip_ref.infolist():\n","                zip_ref.extract(entry, extract_to)\n","                pbar.update(1)\n","\n","# Usage example\n","zip_file_path = '/content/drive/MyDrive/ML_Colab/AV/PAR_DATASET/Validation/validation_set.zip'\n","extract_to_directory = '/content/drive/MyDrive/ML_Colab/AV/Dataset'\n","\n","unzip_file(zip_file_path, extract_to_directory)\n"],"metadata":{"id":"2BGjih6lcRSJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tyFJqycAXAR_"},"source":["#ParDataset creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LF1bvh-I8Sgn"},"outputs":[],"source":["# Custom Dataset class for creating a dataset of pairs (Image,labels)\n","# During the creation are selected only the images with all labels, and are applied\n","# data transformation for the compatibility with the chosed pretrained network\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","import torch\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, root_dir, txt_file, transform=None, max_images=1000):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.max_images = max_images\n","\n","        with open(txt_file, 'r') as f:\n","            lines = f.readlines()\n","\n","        self.data = []\n","        loaded_images = 0\n","\n","        for line in lines:\n","            line = line.strip().split(',')\n","            image_name = line[0]\n","\n","            # Prendi solo le etichette che non sono -1\n","            if int(line[1]) != -1 and int(line[2]) != -1 and int(line[3]) != -1 and int(line[4]) != -1 and int(line[5]) != -1:\n","                labels = [int(x) for x in line[1:6]]  # Converti le etichette in int e crea una lista\n","                # Sottrai -1 solo alle prime due etichette\n","                labels[0] -= 1\n","                labels[1] -= 1\n","\n","                labels = torch.tensor(labels)  # Converti la lista di etichette in un tensore\n","            else:\n","                continue  # Passa alla successiva riga se ci sono etichette -1 o raggiunto il limite massimo di immagini\n","\n","            img_path = os.path.join(self.root_dir, image_name)\n","\n","            try:\n","                image = Image.open(img_path)\n","\n","                # Applica la trasformazione qui\n","                if self.transform:\n","                    image = self.transform(image)\n","\n","            except Exception as e:\n","                # print(f\"Errore durante l'apertura o trasformazione dell'immagine {img_path}: {e}\")\n","                continue\n","\n","            # Usa tutte le etichette valide\n","            self.data.append((image, labels))\n","            loaded_images += 1\n","\n","            if loaded_images == self.max_images:\n","                break\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        image, labels = self.data[idx]\n","        return image, labels\n","\n","\n"]},{"cell_type":"code","source":["# Training Dataset of 5000 samples\n","# Directory contenente le immagini\n","root_dir = '/content/drive/MyDrive/ML_Colab/AV/Dataset/training_set'\n","label_file = '/content/drive/MyDrive/ML_Colab/AV/PAR_DATASET/Training/training_set.txt'\n","\n","# Esempio di trasformazioni che puoi applicare alle immagini (modifica a seconda delle tue esigenze)\n","import torchvision.transforms as transforms\n","\n","data_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Cambia la dimensione dell'immagine a 224x224\n","    transforms.ToTensor(),  # Converte l'immagine in un tensore\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizza l'immagine\n","])\n","\n","# Creazione dell'istanza del dataset con massimo 1000 immagini\n","custom_dataset = CustomDataset(root_dir, label_file, transform=data_transform, max_images=5000)"],"metadata":{"id":"niPkKYesZXau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Script to calculate the number of elements for each class used to calculate the weighted losses\n","\n","\n","# Initialize color counts\n","black, blue, brown, gray, green, orange, pink, purple, red, white, yellow = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n","\n","# Initialize label counts for other attributes\n","l_black, l_blue, l_brown, l_gray, l_green, l_orange, l_pink, l_purple, l_red, l_white, l_yellow = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n","male, female = 0, 0\n","bag, no_bag = 0, 0\n","hat, no_hat = 0, 0\n","\n","# Iterate through the dataset\n","for i in range(len(custom_dataset)):\n","    image, labels = custom_dataset[i]\n","\n","    # Update color counts\n","    if labels[0] == 0:\n","        black += 1\n","    elif labels[0] == 1:\n","        blue += 1\n","    elif labels[0] == 2:\n","        brown += 1\n","    elif labels[0] == 3:\n","        gray += 1\n","    elif labels[0] == 4:\n","        green += 1\n","    elif labels[0] == 5:\n","        orange += 1\n","    elif labels[0] == 6:\n","        pink += 1\n","    elif labels[0] == 7:\n","        purple += 1\n","    elif labels[0] == 8:\n","        red += 1\n","    elif labels[0] == 9:\n","        white += 1\n","    elif labels[0] == 10:\n","        yellow += 1\n","\n","    # Update counts for other attributes\n","    if labels[1] == 0:\n","        l_black += 1\n","    elif labels[1] == 1:\n","        l_blue += 1\n","    elif labels[1] == 2:\n","        l_brown += 1\n","    elif labels[1] == 3:\n","        l_gray += 1\n","    elif labels[1] == 4:\n","        l_green += 1\n","    elif labels[1] == 5:\n","        l_orange += 1\n","    elif labels[1] == 6:\n","        l_pink += 1\n","    elif labels[1] == 7:\n","        l_purple += 1\n","    elif labels[1] == 8:\n","        l_red += 1\n","    elif labels[1] == 9:\n","        l_white += 1\n","    elif labels[1] == 10:\n","        l_yellow += 1\n","\n","    if labels[2] == 0:\n","      male += 1\n","    if labels[2] == 1:\n","      female += 1\n","\n","    if labels[3] == 0:\n","      no_bag += 1\n","    if labels[3] == 1:\n","      bag += 1\n","\n","    if labels[4] == 0:\n","      no_hat += 1\n","    if labels[4] == 1:\n","      hat += 1\n","\n","# Print the counts\n","print(\"Color Counts:\")\n","print(f\"Black: {black}, Blue: {blue}, Brown: {brown}, Gray: {gray}, Green: {green}, Orange: {orange}, Pink: {pink}, Purple: {purple}, Red: {red}, White: {white}, Yellow: {yellow}\")\n","\n","print(\"\\nLabel Counts:\")\n","print(f\"L_Black: {l_black}, L_Blue: {l_blue}, L_Brown: {l_brown}, L_Gray: {l_gray}, L_Green: {l_green}, L_Orange: {l_orange}, L_Pink: {l_pink}, L_Purple: {l_purple}, L_Red: {l_red}, L_White: {l_white}, L_Yellow: {l_yellow}\")\n","print(f\"Male: {male}, Female: {female}\")\n","print(f\"Bag: {bag}, No Bag: {no_bag}\")\n","print(f\"Hat: {hat}, No Hat: {no_hat}\")\n"],"metadata":{"id":"Ov48qkbH_z2t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706196390934,"user_tz":-60,"elapsed":592,"user":{"displayName":"MARCO MILONE","userId":"17232021698904427689"}},"outputId":"3c88df43-1328-4642-e29d-0af2313bcfef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Color Counts:\n","Black: 2814, Blue: 348, Brown: 111, Gray: 586, Green: 142, Orange: 43, Pink: 91, Purple: 71, Red: 334, White: 349, Yellow: 111\n","\n","Label Counts:\n","L_Black: 3331, L_Blue: 1228, L_Brown: 23, L_Gray: 298, L_Green: 29, L_Orange: 1, L_Pink: 1, L_Purple: 0, L_Red: 16, L_White: 22, L_Yellow: 51\n","Male: 3782, Female: 1218\n","Bag: 942, No Bag: 4058\n","Hat: 130, No Hat: 4870\n"]}]},{"cell_type":"markdown","metadata":{"id":"I1LECl0XXqk_"},"source":["#Model Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJHuRwOq-sbS"},"outputs":[],"source":["# Definition of the multitask network built using the mobileNetV2 network as a shared basic architecture\n","# freezing the weights up to the second to last level and adding an output head for each task.\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","class MultiTaskMobileNetV2(nn.Module):\n","    def __init__(self, num_output_gender, num_output_hat, num_output_bag, num_output_top_color, num_output_bottom_color):\n","        super(MultiTaskMobileNetV2, self).__init__()\n","\n","        # Load the pre-trained MobileNetV2 shared base\n","        self.base_model = models.mobilenet_v2(pretrained=True)\n","\n","        # Freeze all layers up to the second-to-last layer\n","        for name, param in self.base_model.named_parameters():\n","            if not name.startswith('classifier') and not name.startswith('features.18'):  # Ignore the last fully connected layer\n","                param.requires_grad = False\n","\n","        # Modify the last fully connected layer to match the tasks\n","        self.base_model.classifier[-1] = nn.Identity()\n","\n","        # Add task-specific branches for each task\n","        self.gender_fc = nn.Linear(1280, num_output_gender)\n","        self.hat_fc = nn.Linear(1280, num_output_hat)\n","        self.bag_fc = nn.Linear(1280, num_output_bag)\n","        self.top_color_fc = nn.Linear(1280, num_output_top_color)\n","        self.bottom_color_fc = nn.Linear(1280, num_output_bottom_color)\n","\n","    def forward(self, x):\n","        # Pass through the shared MobileNetV2 base\n","        shared_features = self.base_model(x)\n","\n","        # Output for each task\n","        output_gender = self.gender_fc(shared_features)\n","        output_hat = self.hat_fc(shared_features)\n","        output_bag = self.bag_fc(shared_features)\n","        output_top_color = self.top_color_fc(shared_features)\n","        output_bottom_color = self.bottom_color_fc(shared_features)\n","\n","        return output_gender, output_hat, output_bag, output_top_color, output_bottom_color\n","\n","\n"]},{"cell_type":"code","source":["# Model Creation\n","model = MultiTaskMobileNetV2(num_output_gender=1, num_output_hat=1, num_output_bag=1, num_output_top_color=11, num_output_bottom_color=11)\n","model"],"metadata":{"id":"-ghe1Mm7JEwI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NKxfVEmFX_GC"},"source":["#Cross-validation and dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDlUUouwX7Y2"},"outputs":[],"source":["from torch.utils.data import Subset, DataLoader\n","from torch.utils.data import random_split\n","\n","# Function used for the creation of folder for K-cross validation\n","def cross_val_dataloaders(train_dataset, val_dataset=None, K=10, batch_size=32):\n","  if val_dataset is None:\n","    val_dataset = train_dataset\n","\n","  dataloader_params = {\"batch_size\": batch_size, \"num_workers\": 2, \"pin_memory\": True}\n","  if K == 1:\n","          # If K = 1, perform a division of the data into training and validation using a specified proportion\n","          proportion = 0.2\n","          total_length = len(train_dataset)\n","          val_length = int(total_length * proportion)\n","          train_length = total_length - val_length\n","\n","          train_dataset, val_dataset = random_split(train_dataset, [train_length, val_length])\n","\n","          # Creazione dei DataLoader per training e validation\n","          train_fold = DataLoader(train_dataset, shuffle=True, **dataloader_params)\n","          val_fold = DataLoader(val_dataset, shuffle=False, **dataloader_params)\n","\n","          return [train_fold], [val_fold], None\n","  else:\n","    indexes = torch.randperm(len(train_dataset)) % K # Random sequence of unique indices with respect to module K\n","    train_folds, val_folds = [], []\n","    for k in range(K):\n","\n","        val_fold   = Subset(val_dataset,   (indexes==k).nonzero().squeeze())\n","        train_fold = Subset(train_dataset, (indexes!=k).nonzero().squeeze())\n","\n","        val_fold   = DataLoader(val_fold,   shuffle=False, **dataloader_params) # Shuffle False because it is important in validation to maintain order to evaluate the model\n","        train_fold = DataLoader(train_fold, shuffle=True,  **dataloader_params) # Shuffle to true can help prevent the model from fitting to specific sequences and improve generalization\n","\n","        val_folds.append(val_fold)\n","        train_folds.append(train_fold)\n","\n","    return train_folds, val_folds, indexes"]},{"cell_type":"markdown","metadata":{"id":"7_JKJLzCYFPu"},"source":["# Training monitoring"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7ynBXty-5kY"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","from tensorboard import notebook\n","\n","def start_tensorboard(log_dir):\n","  writer = SummaryWriter(os.path.join(\"runs\", log_dir))\n","\n","  # run tensorboard in background\n","  ! killall tensorboard\n","  %load_ext tensorboard\n","  %tensorboard --logdir ./runs\n","\n","  notebook.list() # View open TensorBoard instances\n","\n","  return writer"]},{"cell_type":"markdown","metadata":{"id":"8vTfU1fjYc9C"},"source":["#Training and validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDQ0pgLS_zmg"},"outputs":[],"source":["import torch\n","# Definition of one epoch of trainig computing training e validation loss and accuracy\n","def one_epoch(model, lossFunctions, output_activations, optimizer, train_loader, val_loader, writer, epoch, device):\n","    model.to(device)\n","    model.train()\n","\n","    i_start = epoch * len(train_loader)\n","\n","    for i, (X, y) in tqdm(enumerate(train_loader), desc=f\"Epoch {epoch} - Train\"):\n","        if i == 0:\n","            writer.add_image('first_batch', torchvision.utils.make_grid(X.squeeze()))\n","\n","        X = X.squeeze().to(device)\n","        y_top_color = y[:, 0].clone().to(device).long()\n","        y_bottom_color = y[:, 1].clone().detach().to(device).long()\n","        y_gender = y[:, 2].clone().detach().to(device).float()\n","        y_hat = y[:, 3].clone().detach().to(device).float()\n","        y_bag = y[:, 4].clone().detach().to(device).float()\n","\n","\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        o_gender, o_hat, o_bag, o_top_color, o_bottom_color = model(X)\n","        o_gender, o_hat, o_bag, o_top_color, o_bottom_color = output_activations[0](o_gender).squeeze(), output_activations[1](o_hat).squeeze(), output_activations[2](o_bag).squeeze(), output_activations[3](o_top_color).squeeze(), output_activations[4](o_bottom_color).squeeze()\n","\n","        # Compute losses for each task\n","        l_gender = lossFunctions[0](o_gender, y_gender)\n","        l_hat = lossFunctions[1](o_hat, y_hat)\n","        l_bag = lossFunctions[2](o_bag, y_bag)\n","        l_top_color = lossFunctions[3](o_top_color, y_top_color)\n","        l_bottom_color = lossFunctions[4](o_bottom_color, y_bottom_color)\n","\n","        # Compute the total loss (you may weigh the losses if needed)\n","        total_loss = l_gender + l_hat + l_bag + l_top_color + l_bottom_color\n","\n","        # Backward pass\n","        total_loss.backward()\n","        optimizer.step()\n","\n","        acc_g = ((o_gender.detach() > .5) == y_gender.detach()).float().mean()\n","        acc_h = ((o_hat.detach() > .5) == y_hat.detach()).float().mean()\n","        acc_b = ((o_bag.detach() > .5) == y_bag.detach()).float().mean()\n","\n","        # Trova la classe predetta\n","        predicted_classes_tc = torch.argmax(o_top_color, dim=1)\n","        predicted_classes_bc = torch.argmax(o_bottom_color, dim=1)\n","\n","        acc_tc = (predicted_classes_tc == y_top_color).float().mean()\n","        acc_bc = (predicted_classes_bc == y_bottom_color).float().mean()\n","\n","        total_accuracy = (acc_g + acc_h + acc_b + acc_tc + acc_bc) / 5\n","\n","        print(\"- batch loss and accuracy : {:.7f}\\t{:.4f}\".format(total_loss.detach().item(), total_accuracy))\n","        writer.add_scalar('train/loss', total_loss.detach().item(), i_start + i)\n","        writer.add_scalar('train/acc', total_accuracy, i_start + i)\n","\n","    model.eval()  # Set up the model for evaluation\n","    with torch.no_grad():\n","        val_loss = []\n","        val_acc_g = []\n","        val_acc_h = []\n","        val_acc_b = []\n","        val_acc_tc = []\n","        val_acc_bc = []\n","\n","        for X, y in tqdm(val_loader, desc=\"epoch {} - validation\".format(epoch)):\n","            X = X.squeeze().to(device)\n","            y_top_color = y[:, 0].clone().to(device).long()\n","            y_bottom_color = y[:, 1].clone().detach().to(device).long()\n","            y_gender = y[:, 2].clone().detach().to(device).float()\n","            y_hat = y[:, 3].clone().detach().to(device).float()\n","            y_bag = y[:, 4].clone().detach().to(device).float()\n","\n","            # Forward pass\n","            o_gender, o_hat, o_bag, o_top_color, o_bottom_color = model(X)\n","            o_gender, o_hat, o_bag, o_top_color, o_bottom_color = output_activations[0](o_gender).squeeze(), output_activations[1](o_hat).squeeze(), output_activations[2](o_bag).squeeze(), output_activations[3](o_top_color).squeeze(), output_activations[4](o_bottom_color).squeeze()\n","\n","            # Compute losses for each task\n","            l_gender = lossFunctions[0](o_gender, y_gender)\n","            l_hat = lossFunctions[1](o_hat, y_hat)\n","            l_bag = lossFunctions[2](o_bag, y_bag)\n","            l_top_color = lossFunctions[3](o_top_color, y_top_color)\n","            l_bottom_color = lossFunctions[4](o_bottom_color, y_bottom_color)\n","\n","            # Compute the total loss (you may weigh the losses if needed)\n","            total_loss = l_gender + l_hat + l_bag + l_top_color + l_bottom_color\n","\n","            val_loss.append(total_loss)\n","\n","            # Compute accuracies for each task\n","            acc_g = ((o_gender > 0.5) == y_gender).float().mean().item()\n","            acc_h = ((o_hat > 0.5) == y_hat).float().mean().item()\n","            acc_b = ((o_bag > 0.5) == y_bag).float().mean().item()\n","            acc_tc = (torch.argmax(o_top_color, dim=1) == y_top_color).float().mean().item()\n","            acc_bc = (torch.argmax(o_bottom_color, dim=1) == y_bottom_color).float().mean().item()\n","\n","            val_acc_g.append(acc_g)\n","            val_acc_h.append(acc_h)\n","            val_acc_b.append(acc_b)\n","            val_acc_tc.append(acc_tc)\n","            val_acc_bc.append(acc_bc)\n","\n","        val_loss = torch.stack(val_loss).mean().item()\n","        val_acc_g = sum(val_acc_g) / len(val_acc_g)\n","        val_acc_h = sum(val_acc_h) / len(val_acc_h)\n","        val_acc_b = sum(val_acc_b) / len(val_acc_b)\n","        val_acc_tc = sum(val_acc_tc) / len(val_acc_tc)\n","        val_acc_bc = sum(val_acc_bc) / len(val_acc_bc)\n","        total_accuracy = (val_acc_g + val_acc_h + val_acc_b + val_acc_tc + val_acc_bc) / 5\n","\n","        print(\"Validation loss and accuracy : {:.7f}\\t{:.4f}\".format(val_loss, total_accuracy))\n","        writer.add_scalar('val/loss', val_loss, i_start + i)\n","        writer.add_scalar('val/acc', total_accuracy, i_start + i)\n","\n","    return val_loss, total_accuracy\n"]},{"cell_type":"markdown","metadata":{"id":"ebVg4V09YjIh"},"source":["# Run the code"]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch\n","\n","# Weighted losses used to address class imbalance in the training data.\n","# By assigning different weights to different classes based on their distribution,\n","# the model is encouraged to focus more on under-represented classes during training.\n","\n","# Definition of class distributions\n","class_counts_color = torch.tensor([black, blue, brown, gray, green, orange, pink, purple, red, white, yellow], dtype=torch.float)\n","class_counts_label = torch.tensor([l_black, l_blue, l_brown, l_gray, l_green, l_orange, l_pink, l_purple, l_red, l_white, l_yellow], dtype=torch.float)\n","class_counts_gender = torch.tensor([male, female], dtype=torch.float)\n","class_counts_bag = torch.tensor([bag, no_bag], dtype=torch.float)\n","class_counts_hat = torch.tensor([hat, no_hat], dtype=torch.float)\n","\n","# Normalization of class distributions\n","class_weights_color = class_counts_color / class_counts_color.sum()\n","class_weights_label = class_counts_label / class_counts_label.sum()\n","class_weights_gender = class_counts_gender / class_counts_gender.sum()\n","class_weights_bag = class_counts_bag / class_counts_bag.sum()\n","class_weights_hat = class_counts_hat / class_counts_hat.sum()\n","\n","# Definition of weighted losses\n","weighted_loss_color = nn.CrossEntropyLoss(weight=1 / class_weights_color)\n","weighted_loss_label = nn.CrossEntropyLoss(weight=1 / class_weights_label)\n","weighted_loss_gender = nn.BCEWithLogitsLoss(pos_weight=class_weights_gender[1] / class_weights_gender[0])\n","weighted_loss_bag = nn.BCEWithLogitsLoss(pos_weight=class_weights_bag[1] / class_weights_bag[0])\n","weighted_loss_hat = nn.BCEWithLogitsLoss(pos_weight=class_weights_hat[1] / class_weights_hat[0])\n","\n","\n","\n"],"metadata":{"id":"-t_fVoZzP7Mj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Parameter definition\n"],"metadata":{"id":"S1yNqVY2z4gD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5lNGtSw_4p5"},"outputs":[],"source":["import torch\n","from torch.nn import BCELoss,Sigmoid\n","\n","import torch.nn as nn\n","\n","# Loss functions\n","binary_classification_loss = nn.BCELoss()\n","multiclass_classification_loss = nn.CrossEntropyLoss()\n","\n","# Output layer for each task\n","output_activation_gender = nn.Sigmoid()  # Funzione di attivazione sigmoide per la classificazione binaria\n","output_activation_hat = nn.Sigmoid()  # Funzione di attivazione sigmoide per la classificazione binaria\n","output_activation_bag = nn.Sigmoid()  # Funzione di attivazione sigmoide per la classificazione binaria\n","output_activation_top_color = nn.Softmax(dim=1)  # Funzione di attivazione Softmax per la classificazione multiclasse\n","output_activation_bottom_color = nn.Softmax(dim=1)  # Funzione di attivazione Softmax per la classificazione multiclasse\n","\n","# List of loss\n","#lossFunctions = [binary_classification_loss, binary_classification_loss, binary_classification_loss, multiclass_classification_loss, multiclass_classification_loss]\n","# Loss weighted\n","lossFunctions = [weighted_loss_gender,weighted_loss_bag, weighted_loss_hat,weighted_loss_color,weighted_loss_label]\n","output_activations = [output_activation_gender, output_activation_hat, output_activation_bag, output_activation_top_color, output_activation_bottom_color]\n","\n","batch_size = 64\n","lr = .001\n","momentum = .9\n","lambda_reg = 0\n","\n","epochs = 40\n","early_stopping_patience = 4\n","\n","K_cross_val = 1\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# network parameters\n","num_output = 1 #Number of classes of the classification problem\n","\n","# create output directory and logger\n","experiment_name = 'MTNN'\n","import os\n","os.makedirs(experiment_name)\n","writer = start_tensorboard(experiment_name)"]},{"cell_type":"markdown","source":["Training"],"metadata":{"id":"O6VJ3hg9z9oc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANty3lMM88xB"},"outputs":[],"source":["import torch\n","import torchvision\n","from tqdm import tqdm\n","\n","\n","# Creating folders and saving the index in memory to optionally restore it\n","train_folds, val_folds, indexes = cross_val_dataloaders(custom_dataset,None, K_cross_val, batch_size)\n","torch.save(indexes, os.path.join(experiment_name, \"cross-val-indexes.pt\"))\n","\n","# Initialize variables with relative size\n","val_losses = torch.zeros(epochs, K_cross_val)\n","val_accuracies = torch.zeros(epochs, K_cross_val)\n","\n","\n","for k in range(K_cross_val):\n","\n","  # dataloader, network, optimizer for each fold\n","  train_loader, val_loader = train_folds[k], val_folds[k]\n","  model = MultiTaskMobileNetV2(num_output_gender=1, num_output_hat=1, num_output_bag=1, num_output_top_color=11, num_output_bottom_color=11)\n","  model = model.to(device)\n","\n","  # Ottieni tutti i parametri della rete, inclusi quelli dei rami specifici per ciascun compito\n","  all_parameters = list(model.base_model.parameters()) + \\\n","                  list(model.gender_fc.parameters()) + \\\n","                  list(model.hat_fc.parameters()) + \\\n","                  list(model.bag_fc.parameters()) + \\\n","                  list(model.top_color_fc.parameters()) + \\\n","                  list(model.bottom_color_fc.parameters())\n","\n","  # Definisci l'ottimizzatore utilizzando tutti i parametri\n","  optimizer = torch.optim.SGD(all_parameters,\n","                              lr=lr,\n","                              weight_decay=lambda_reg,\n","                              momentum=momentum)\n","\n","\n","\n","  # early stopping and best model saving\n","  early_stopping_counter = early_stopping_patience\n","  min_val_loss = 1e10\n","\n","  # training and validation\n","  for e in range(epochs):\n","    print(\"FOLD {} - EPOCH {}\".format(k, e))\n","\n","    val_loss, val_accuracy = one_epoch(model, lossFunctions,output_activations, optimizer, train_loader, val_loader, writer,e,device)\n","\n","    # store the validation metrics\n","    val_losses[e, k] = val_loss\n","    val_accuracies[e, k] = val_accuracy\n","    torch.save(val_losses, os.path.join(experiment_name,'val_losses.pth'))\n","    torch.save(val_accuracies, os.path.join(experiment_name,'val_accuracies.pth'))\n","\n","    # save the best model and check the early stopping criteria\n","    if val_loss < min_val_loss: # save the best model\n","      min_val_loss = val_loss\n","      early_stopping_counter = early_stopping_patience # reset early stopping counter\n","      torch.save(model.state_dict(), os.path.join(experiment_name,'fold_{}_best_model.pth'.format(k)))\n","      print(\"- saved best model\")\n","\n","    if e>0: # early stopping counter update\n","      if val_losses[e, k] > val_losses[e-1, k]:\n","          early_stopping_counter -= 1 # update early stopping counter\n","      else:\n","          early_stopping_counter = early_stopping_patience # reset early stopping counter\n","    if early_stopping_counter == 0: # early stopping\n","        break"]},{"cell_type":"markdown","source":["#Testing"],"metadata":{"id":"Vj5njdQaz_tN"}},{"cell_type":"code","source":["# Creation of Test Dataset of 500 samples\n","import torchvision.transforms as transforms\n","\n","data_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Cambia la dimensione dell'immagine a 224x224\n","    transforms.ToTensor(),  # Converte l'immagine in un tensore\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizza l'immagine\n","])\n","\n","root_dir = '/content/drive/MyDrive/ML_Colab/AV/Dataset/validation_set'\n","label_file = '/content/drive/MyDrive/ML_Colab/AV/PAR_DATASET/Validation/validation_set.txt'\n","\n","\n","# Creazione dell'istanza del dataset con massimo 1000 immagini\n","Test_Set = CustomDataset(root_dir, label_file, transform=data_transform, max_images=500)"],"metadata":{"id":"ieDSC7wSX1jF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculation of the accuracy for each task\n","import torch\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","import random\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","# Definisci il tuo modello MultiTaskMobileNetV2\n","model = MultiTaskMobileNetV2(num_output_gender=1, num_output_hat=1, num_output_bag=1, num_output_top_color=11, num_output_bottom_color=11)\n","weights_path = '/content/drive/MyDrive/ML_Colab/AV/MultiTask_w_loss/fold_0_best_model.pth'\n","model.load_state_dict(torch.load(weights_path))\n","model.eval()\n","\n","# Livelli di output per ciascun compito\n","output_activation_gender = nn.Sigmoid()  # Funzione di attivazione sigmoide per la classificazione binaria\n","output_activation_hat = nn.Sigmoid()  # Funzione di attivazione sigmoide per la classificazione binaria\n","output_activation_bag = nn.Sigmoid()  # Funzione di attivazione sigmoide per la classificazione binaria\n","output_activation_top_color = nn.Softmax(dim=1)  # Funzione di attivazione Softmax per la classificazione multiclasse\n","output_activation_bottom_color = nn.Softmax(dim=1)  # Funzione di attivazione Softmax per la classificazione multiclasse\n","output_activations = [output_activation_gender, output_activation_hat, output_activation_bag, output_activation_top_color, output_activation_bottom_color]\n","\n","\n","correct_gender = 0\n","correct_hat = 0\n","correct_bag = 0\n","correct_top_color = 0\n","correct_bottom_color = 0\n","\n","total_samples = len(Test_Set)\n","\n","with torch.no_grad():\n","\n","    for X, y in Test_Set:\n","\n","        X = X.unsqueeze(0)\n","        y_top_color = y[0]\n","        y_bottom_color = y[1]\n","        y_gender = y[2]\n","        y_hat = y[3]\n","        y_bag = y[4]\n","\n","        # Forward pass\n","        o_gender, o_hat, o_bag, o_top_color, o_bottom_color = model(X)\n","        o_gender, o_hat, o_bag, o_top_color, o_bottom_color = output_activations[0](o_gender).squeeze(), output_activations[1](o_hat).squeeze(), output_activations[2](o_bag).squeeze(), output_activations[3](o_top_color).squeeze(), output_activations[4](o_bottom_color).squeeze()\n","\n","        # Calcola l'accuratezza binaria per i compiti di genere, cappello e borsa\n","        acc_g = ((o_gender.detach() > 0.5) == y_gender.detach())\n","        acc_h = ((o_hat.detach() > 0.5) == y_hat.detach())\n","        acc_b = ((o_bag.detach() > 0.5) == y_bag.detach())\n","\n","        # Trova la classe predetta\n","        predicted_classes_tc = torch.argmax(o_top_color, dim=0)\n","        predicted_classes_bc = torch.argmax(o_bottom_color, dim=0)\n","\n","        # Calcola l'accuratezza di classe per i compiti di colore superiore e inferiore\n","        acc_tc = (predicted_classes_tc == y_top_color)\n","        acc_bc = (predicted_classes_bc == y_bottom_color)\n","\n","        # Aggiorna il numero totale di classificazioni corrette per ogni compito\n","        correct_gender += acc_g.item()\n","        correct_hat += acc_h.item()\n","        correct_bag += acc_b.item()\n","        correct_top_color += acc_tc.item()\n","        correct_bottom_color += acc_bc.item()\n","\n","# Calcola l'accuratezza per ogni compito\n","accuracy_gender = correct_gender / total_samples\n","accuracy_hat = correct_hat / total_samples\n","accuracy_bag = correct_bag / total_samples\n","accuracy_top_color = correct_top_color / total_samples\n","accuracy_bottom_color = correct_bottom_color / total_samples\n","\n","# Stampa i risultati\n","print(\"Accuracy - Gender:\", accuracy_gender)\n","print(\"Accuracy - Hat:\", accuracy_hat)\n","print(\"Accuracy - Bag:\", accuracy_bag)\n","print(\"Accuracy - Top Color:\", accuracy_top_color)\n","print(\"Accuracy - Bottom Color:\", accuracy_bottom_color)\n","\n","\n"],"metadata":{"id":"n-vlZxWPgEEH"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1kvI7e7Ok72MtJbNsj4MADXSY5Qx6UMC5","authorship_tag":"ABX9TyMoSs4KalenylpBcDJjDb/l"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}